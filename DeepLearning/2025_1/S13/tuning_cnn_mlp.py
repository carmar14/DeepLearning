# -*- coding: utf-8 -*-
"""tuning_cnn_mlp.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1MSUZWLZnET9Ekwd9jGDI2XiaJf2KnJ6-
"""

import numpy as np
import keras_tuner
import keras
from keras import layers

(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()
# Normalize the pixel values to the range of [0, 1].
x_train = x_train.astype("float32") / 255
x_test = x_test.astype("float32") / 255
# Add the channel dimension to the images.
x_train = np.expand_dims(x_train, -1)
x_test = np.expand_dims(x_test, -1)
# Print the shapes of the data.
print(x_train.shape)
print(y_train.shape)
print(x_test.shape)
print(y_test.shape)

def build_model(hp):
    inputs = keras.Input(shape=(28, 28, 1))
    # Model type can be MLP or CNN.
    model_type = hp.Choice("model_type", ["mlp", "cnn"])
    x = inputs
    if model_type == "mlp":
        x = layers.Flatten()(x)
        # Number of layers of the MLP is a hyperparameter.
        for i in range(hp.Int("mlp_layers", 1, 3)):
            # Number of units of each layer are
            # different hyperparameters with different names.
            x = layers.Dense(
                units=hp.Int(f"units_{i}", 32, 128, step=32),
                activation="relu",
            )(x)
    else:
        # Number of layers of the CNN is also a hyperparameter.
        for i in range(hp.Int("cnn_layers", min_value=1, # Mínimo número de capas ocultas
                             max_value=3, # Máximo número de capas ocultas
                             step=1 # Tamaño del salto (para obtener 1, 2 o 3
                              )):
            x = layers.Conv2D(
                hp.Int(f"filters_{i}", 32, 128, step=32),
                kernel_size=(3, 3),
                activation="relu",
            )(x)
            x = layers.MaxPooling2D(pool_size=(2, 2))(x)
        x = layers.Flatten()(x)

    # A hyperparamter for whether to use dropout layer.
    if hp.Boolean("dropout"):
        x = layers.Dropout(0.5)(x)

    # The last layer contains 10 units,
    # which is the same as the number of classes.
    outputs = layers.Dense(units=10, activation="softmax")(x)
    model = keras.Model(inputs=inputs, outputs=outputs)

    # Compile the model.
    model.compile(
        loss="sparse_categorical_crossentropy",
        metrics=["accuracy"],
        optimizer="adam",
    )
    return model

"""We can do a quick test of the models to check if it build successfully for both CNN and MLP."""

# Initialize the `HyperParameters` and set the values.
hp = keras_tuner.HyperParameters()
hp.values["model_type"] = "cnn"
hp.Boolean("dropout")
# Build the model using the `HyperParameters`.
model = build_model(hp)
# Test if the model runs with our data.
model(x_train[:100])
# Print a summary of the model.
model.summary()

# Do the same for MLP model.
hp.values["model_type"] = "mlp"
model = build_model(hp)
model(x_train[:100])
model.summary()

tuner = keras_tuner.RandomSearch(
    build_model,
    max_trials=10,
    objective="val_accuracy",
    directory="afinacion_red", # Directorio local donde se almacenarán los resultados de entrenamiento
    project_name="trials", # Subdirectorio donde se almacenarán los resultados
    overwrite = True, # Para sobre-escribir los directorios cada vez que afinemos
)

tuner.search(
    x_train,
    y_train,
    validation_split=0.2,
    epochs=2
)

# Imprimir resumen de la afinación
tuner.results_summary()

# Extraer el mejor modelo
mejor_modelo = tuner.get_best_models(num_models=1)[0]
mejor_modelo.summary()

# model loss and accuracy on validation set
mejor_modelo.evaluate(x_test, y_test, verbose=False)

y_test
print(y_test.shape)

# image processing
import matplotlib.image as mpimg
# for ploting graphs
import matplotlib.pyplot as plt

g = plt.imshow(x_test[0][:,:,0])
print(y_test[0])

# predicted values
y_pred_enc = mejor_modelo.predict(x_test)
print(y_pred_enc.shape)

# actual
#y_act = [np.argmax(i) for i in y_test]

# decoding predicted values
y_pred = [np.argmax(i) for i in y_pred_enc]
y_pred= list(map(int, y_pred))
y_act = y_test

print(y_pred_enc[0])
print(y_pred)
print(y_test)

from sklearn.metrics import classification_report
print(classification_report(y_act, y_pred))

import seaborn as sns
from sklearn.metrics import confusion_matrix
fig, ax = plt.subplots(figsize=(7, 7))
sns.heatmap(confusion_matrix(y_act, y_pred), annot=True,
            cbar=False, fmt='1d', cmap='Blues', ax=ax)
ax.set_title('Confusion Matrix', loc='left', fontsize=16)
ax.set_xlabel('Predicted')
ax.set_ylabel('Actual')
plt.show()

import random

# Generate 10 random indices
indices_aleatorios = random.sample(range(len(y_pred)), 10)

# Get the predicted values for the random indices
numeros_aleatorios = [y_pred[i] for i in indices_aleatorios]

# Get the corresponding actual values
actual_values = [y_act[i] for i in indices_aleatorios]

# Get the corresponding images
images_aleatorias = [x_test[i] for i in indices_aleatorios]


# Plot the images and their predictions
plt.figure(figsize=(15, 6))  # Adjust figure size as needed

for i in range(10):
    plt.subplot(2, 5, i + 1)
    plt.imshow(images_aleatorias[i][:, :, 0], cmap='gray')
    plt.title(f"Pred: {numeros_aleatorios[i]}, Actual: {actual_values[i]}")
    plt.axis('off')

plt.tight_layout()
plt.show()