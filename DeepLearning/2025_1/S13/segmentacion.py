# -*- coding: utf-8 -*-
"""segmentacion.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1YqlIA35mKZDAZIg6h0FAkn8I2_C3e7ni

# SEGMENTACIÓN DE IMÁGENES SATELITALES USANDO REDES CONVOLUCIONALES

Se verá cómo crear, entrenar y poner a prueba la red U-Net, basada en Redes Convolucionales, para segmentar imágenes satelitales.

Contenido:

1. [La segmentación con Redes Convolucionales: U-Net](#scrollTo=p0PgBzu-nuNV&line=1&uniqifier=1)
2. [El set de datos](#scrollTo=nsfWE-mw_Rvv&line=15&uniqifier=1)
3. [Creación y entrenamiento de U-Net](#scrollTo=_2NWkDMngemL&line=12&uniqifier=1)
4. [Segmentación de imágenes con el modelo entrenado](#scrollTo=oH4JBU1Wncu_&line=1&uniqifier=1)

## Aplicaciones de la segmentación

- Vehículos autónomos
- Análisis de imágenes médicas
- Análisis de imágenes satelitales

## 1. La segmentación con Redes Convolucionales: U-Net

Las Redes Convolucionales son la arquitectura ideal para la segmentación de imágenes.

Una de las arquitecturas más usadas es U-Net, desarrollada en el año 2015 para la segmentación de imágenes médicas:

![](https://drive.google.com/uc?export=view&id=1t_Cmqjvd_fnE6zbsa5F3KtxgBOn3pYUf)

El principio de funcionamiento de U-Net es sencillo y se basa en estos componentes:

- **Codificador**: capas convolucionales que aprenden a extraer las características más relevantes de las imágenes de entrada y a codificarlas como un vector. Básicamente detecta las clases.
- **Decodificador**: capas convolucionales **inversas** que permiten **reconstruir** la imagen a partir de la codificación. Lo que se busca en esta parte es encontrar la ubicación de las clases.
- **Skip connections** (conexiones directas): permiten interconectar capas equivalentes entre codificador y decodificador. Es una manera de presentarle al decodificador los detalles locales y globales de las imágenes de entrada. Ayuda a mitigar el efecto del desvanecimiento del gradiente

## 2. El set de datos

Usaremos un [set de datos](https://drive.google.com/drive/folders/1vH9ne6eIFRNGOVROSGbTEDSWuZJrMbI2) que contiene imágenes 5.535 imágenes satelitales con sus correspondientes máscaras de segmentación:

![](https://drive.google.com/uc?export=view&id=1u2l2MF4vmUxIQCCth-qVE1E0GqMDXKUs)

Algunas características:

- Imágenes y máscaras: tamaño de 128x128
- Rango de valores pixeles imágenes: 0 a 255
- Rango de valores pixeles máscaras:
  - *construcción* -> 0
  - *tierra* -> 1
  - *vegetación* -> 2
  - *vía* -> 3
  - *agua* -> 4
  - *otro* -> 5

### 2.1. Lectura del set de datos

Los pasos para poder leer el set de datos son los siguientes:

1. Descargar los archivos (*imgs.zip*, *msks.zip*, *metadata.csv*) localmente desde el [enlace de descarga](https://drive.google.com/drive/folders/1vH9ne6eIFRNGOVROSGbTEDSWuZJrMbI2)
2. Cargarlos en Google Drive
3. Montar Google Drive en Google Colab:
"""

# Montar google drive
#from google.colab import drive
#drive.mount('/content/gdrive',force_remount=True)

"""A continuación descomprimimos los archivos *imgs.zip* (imágenes) y *msks.zip* (máscaras de segmentación), especificando en cada caso la ruta donde están almacenados en Google Drive:"""

# Descargar a disco de la máquina virtual y descomprimir
#!unzip "/content/gdrive/MyDrive/Colab Notebooks/Segmentacion/data/imgs.zip" -d "/content/imgs"
#!unzip "/content/gdrive/MyDrive/Colab Notebooks/Segmentacion/data/msks.zip" -d "/content/msks"

"""Ahora leemos el archivo de meta-datos (*metadata.csv*).

Este archivo tiene el listado completo de imágenes y máscaras que usaremos para crear los datasets de TensorFlow:
"""

# Leer metadatos
import pandas as pd

ruta = "/content/gdrive/MyDrive/Colab Notebooks/Segmentacion/data/metadata.csv"
df = pd.read_csv(ruta)
df



"""### 2.2. Preparación del set de datos

En esta fase se hace lo siguiente:

1. Mezclar aleatoriamente las filas de *metadata.csv*
2. Generar las particiones para entrenamiento, validación y prueba (70%/20%/10%)
3. Crear los datasets de TensorFlow **teniendo en cuenta que aún no leeremos los datos** (para evitar problemas de almacenamiento en la RAM)

Veamos la primera parte de este preprocesamiento. Comencemos mezclando aleatoriamente las filas de *metadata.csv*:
"""

# Mezclar aleatoriamente filas de metadata.csv
df = df.sample(frac=1, random_state=23)
df

"""Ahora calculemos la cantidad de datos de entrenamiento (70%), validación (20%) y prueba (10%):"""

# Tamaños sets train, val, test
N = df.shape[0] # Cantidad total de datos
n_train = int(0.7*N) # Cantidad de datos de entrenamiento
n_val = int(0.2*N) # Cantidad de datos de validación
n_test = N - n_train - n_val # Cantidad de datos de prueba

print(N, n_train, n_val, n_test)

"""Ahora generemos las particiones.

Cada partición será simplemente un arreglo con el listado de nombres de las imágenes (arreglos "x") y de sus correspondientes máscaras (arreglos "y"):
"""

# Arreglos entrenamiento
x_train_fnames = df['imgs'].values[0:n_train]
y_train_fnames = df['msks'].values[0:n_train]

# Arreglos validación
x_val_fnames = df['imgs'].values[n_train:n_train+n_val]
y_val_fnames = df['msks'].values[n_train:n_train+n_val]

# Arreglos prueba
x_test_fnames = df['imgs'].values[n_train+n_val:]
y_test_fnames = df['msks'].values[n_train+n_val:]

for name1, name2 in zip(x_train_fnames, y_train_fnames):
    print(name1,name2)

"""Vemos que estos arreglos no contienen como tal las imágenes/máscaras sino simplemente el nombre de los archivos.

Esto es suficiente para crear el dataset de TensorFlow:
"""

import tensorflow as tf

ds_train = tf.data.Dataset.from_tensor_slices((x_train_fnames, y_train_fnames))
ds_val = tf.data.Dataset.from_tensor_slices((x_val_fnames, y_val_fnames))
ds_test = tf.data.Dataset.from_tensor_slices((x_test_fnames, y_test_fnames))

print(len(ds_train), len(ds_val), len(ds_test))

"""Bien, ya tenemos los datasets de entrenamiento (`ds_train`), validación (`ds_val`) y prueba (`ds_test`).

Verifiquemos que estos dataset no contienen como tal las imágenes/máscaras sino únicamente los nombres de los archivos:
"""

for img_fname, msk_fname in ds_test.take(1):
    print(img_fname.numpy(),msk_fname.numpy())

"""Más adelante, cuando entrenemos el modelo, TensorFlow/Keras realizará la lectura de las imágenes.

Ahora creemos la función `preprocesar_imagen` que será ejecutada durante este entrenamiento. Esta función llevará a cabo este pipeline:

1. Leer las imágenes/máscaras desde los directorios correspondientes
2. Convertir cada imagen del formato *uint8* (enteros 8 bits, sin signo) al formato *float32* (punto flotante) para que sus pixeles estén en la escala de 0 a 1 (en lugar de 0 a 255)
3. Retornar la imagen y la máscara
"""

# Fijar directorios para las imágenes y las máscaras
DIR_IMGS = '/content/imgs/'
DIR_MSKS = '/content/msks/'


# La función para lectura y pre-procesamiento
def preprocesar_imagen(imgpath, mskpath):
    # Leer imagen y máscara
    img = tf.io.read_file(DIR_IMGS + imgpath)
    msk = tf.io.read_file(DIR_MSKS + mskpath)

    # Decodificar imagen con "decode_jpeg" (3 canales)
    # y máscara con "decode_png" (1 canal)
    img = tf.image.decode_jpeg(img, channels=3) #uint8
    msk = tf.image.decode_png(msk, channels=1) #uint8

    # Convertir la imagen a "float32"
    img = tf.image.convert_image_dtype(img, dtype=tf.float32)

    return img, msk

"""Verificar si `preprocesar_imagen` funciona correctamente. Para ello:

- Tomamos el set de validación (`ds_val`)
- Aplicamos la función `preprocesar_imagen` con ayuda de `map()`
- Tomamos un dato con `take()` y mostramos tanto la imagen como la máscara
"""

# Extraer la primera imagen del set de entrenamiento
ds_mini = ds_val.map(preprocesar_imagen).take(5)

# Imprimir tamaño y categoría
for img, msk in ds_mini:
    print(img.numpy().shape)
    print(msk.numpy().shape)

# Y graficarlas
import matplotlib.pyplot as plt

plt.figure(figsize=(4,4))

plt.subplot(2,1,1)
plt.imshow(img)
plt.axis('off')
plt.colorbar()

plt.subplot(2,1,2)
plt.imshow(msk)
plt.axis('off')
plt.colorbar();

"""Veamos en detalle la máscara de segmentación:"""

# Máscara en detalle
import plotly.express as px

fig = px.imshow(msk.numpy().squeeze())
fig.show()

"""En este caso la imagen contiene 5 categorías

Con esto ya estamos listos para implementar la Red U-Net.

## 3. Creación y entrenamiento de U-Net

Usaremos TensorFlow/Keras para crear una versión modificada de la Red U-Net. Nuestra red tendrá estas características:

| **Bloque** | **Características** |
|----------------------|------------------------------------------------------|
| Entrada | Input, 128x128x3 |
| Codificador | - **conv1**: Conv2D, 16 filtros, 3x3, ReLU (x2)<br> - **pool1**: Maxpooling, 2x2<br><br>- **conv2**Conv2D, 32 filtros, 3x3, ReLU (x2)<br>- **pool2**: Maxpooling, 2x2<br><br>- **conv3**: Conv2D, 64 filtros, 3x3, ReLU (x2)<br> - **pool3**: Maxpooling, 2x2<br><br>- **conv4**: Conv2D, 128 filtros, 3x3, ReLU (x2)<br>- **pool4**: Maxpooling, 2x2<br><br>- **conv5**: Conv2D, 256 filtros, 3x3, ReLU (x2)|
| Decodificador | - **dec1**: Conv2DTranspose, 128 filtros, 2x2<br> - **dec1**: concatenate(dec1, conv4)<br>- **dec1**: Conv2DTranspose, 128 filtros, 3x3 (x2)<br><br> - **dec2**: Conv2DTranspose, 64 filtros, 2x2<br> - **dec2**: concatenate(dec2,conv3)<br> - **dec2**: Conv2DTranspose, 64 filtros, 3x3 (x2)<br><br> - **dec3**: Conv2DTranspose, 32 filtros, 2x2<br> - **dec3**: concatenate(dec3,conv2)<br> - **dec3**: Conv2DTranspose, 32 filtros, 3x3 (x2)<br><br> - **dec4**: Conv2DTranspose, 16 filtros, 2x2<br> - **dec4**: concatenate(dec4,conv1)<br> - **dec4**:  - Conv2DTranspose, 16 filtros, 3x3 (x2)<br><br>
| Salida | Conv2D, 6 filtros, 1x1, softmax

Veamos entonces cómo implementar la red en TensorFlow/Keras:
"""

from tensorflow.keras import layers

# Fijar la semilla
tf.random.set_seed(123)

# Tamaños entrada y número de categorías
img_size = (128,128,3) # El tamaño de cada patch
nclases = 6

# ===============
# Entrada
entrada = tf.keras.Input(shape=img_size)

# ===============
# Codificador

# conv1
conv1 = layers.Conv2D(16,(3,3),activation='relu',padding='same')(entrada)
conv1 = layers.Conv2D(16,(3,3),activation='relu',padding='same')(conv1)
pool1 = layers.MaxPool2D((2,2))(conv1)

# conv2
conv2 = layers.Conv2D(32,(3,3),activation='relu',padding='same')(pool1)
conv2 = layers.Conv2D(32,(3,3),activation='relu',padding='same')(conv2)
pool2 = layers.MaxPool2D((2,2))(conv2)

# conv3
conv3 = layers.Conv2D(64,(3,3),activation='relu',padding='same')(pool2)
conv3 = layers.Conv2D(64,(3,3),activation='relu',padding='same')(conv3)
pool3 = layers.MaxPool2D((2,2))(conv3)

# conv4
conv4 = layers.Conv2D(128,(3,3),activation='relu',padding='same')(pool3)
conv4 = layers.Conv2D(128,(3,3),activation='relu',padding='same')(conv4)
pool4 = layers.MaxPool2D((2,2))(conv4)

# conv5
conv5 = layers.Conv2D(256,(3,3),activation='relu',padding='same')(pool4)
conv5 = layers.Conv2D(256,(3,3),activation='relu',padding='same')(conv5)

# ===============
# Decodificador

# dec1
dec1 = layers.Conv2DTranspose(128, (2,2), strides=(2,2), padding='same')(conv5)
dec1 = layers.concatenate([dec1,conv4])
dec1 = layers.Conv2DTranspose(128, (3,3), activation='relu', padding='same')(dec1)
dec1 = layers.Conv2DTranspose(128, (3,3), activation='relu', padding='same')(dec1)

# dec2
dec2 = layers.Conv2DTranspose(64, (2,2), strides=(2,2), padding='same')(dec1)
dec2 = layers.concatenate([dec2,conv3])
dec2 = layers.Conv2DTranspose(64, (3,3), activation='relu', padding='same')(dec2)
dec2 = layers.Conv2DTranspose(64, (3,3), activation='relu', padding='same')(dec2)

# dec3
dec3 = layers.Conv2DTranspose(32, (2,2), strides=(2,2), padding='same')(dec2)
dec3 = layers.concatenate([dec3,conv2])
dec3 = layers.Conv2DTranspose(32, (3,3), activation='relu', padding='same')(dec3)
dec3 = layers.Conv2DTranspose(32, (3,3), activation='relu', padding='same')(dec3)

# dec4
dec4 = layers.Conv2DTranspose(16, (2,2), strides=(2,2), padding='same')(dec3)
dec4 = layers.concatenate([dec4,conv1])
dec4 = layers.Conv2DTranspose(16, (3,3), activation='relu', padding='same')(dec4)
dec4 = layers.Conv2DTranspose(16, (3,3), activation='relu', padding='same')(dec4)

# ===============
# Salida
salida = layers.Conv2D(filters=6, kernel_size=(1,1), activation='softmax')(dec4)

# ===============
# Interconectar todo en un modelo
unet = tf.keras.models.Model(inputs=entrada, outputs=salida)
unet.summary()

"""Habiendo creado el modelo el siguiente paso es compilarlo, es decir definir el optimizador y la pérdida a optimizar.

Como optimizador usaremos Adam (`optimizer = 'adam'`) y como función de error usaremos la entropía cruzada (`loss = 'sparse_categorical_crossentropy'`):
"""

from tensorflow import keras
unet.compile(
    optimizer=keras.optimizers.Adam(learning_rate=1e-3),
    loss = 'sparse_categorical_crossentropy',
    metrics=['accuracy']
)

"""Ahora a entrenar el modelo.

Primero tenemos que crear lotes (*batches*) de los sets de entrenamiento y validación, para evitar leer todos los datos y tener problemas de memoria RAM.

Usaremos lotes de 16 datos y los crearemos con el método `batch`:
"""

# Definir el batch size
BATCH_SIZE_TRAIN = 32
BATCH_SIZE_VAL = 32

# Crear los lotes (aún no leemos las imágenes)
train_batch = ds_train.map(preprocesar_imagen).batch(BATCH_SIZE_TRAIN)
val_batch = ds_val.map(preprocesar_imagen).batch(BATCH_SIZE_VAL)

"""Y con esto ya podemos entrenar el modelo usando sólo una línea de código y el método `fit()`.

Se sugiere usar la GPU y al menos 20 iteraciones de entrenamiento:
"""

# Y entrenar el modelo: ¡en este punto SÍ se leen las imágenes!
history = unet.fit(train_batch,
         validation_data = val_batch,
         epochs=20,
         verbose=2)

"""## 4. Segmentación de imágenes con el modelo entrenado

Habiendo entrenado el modelo lo único que nos falta es ponerlo a prueba.

Para ello usaremos el set de prueba (`ds_test`) que contiene imágenes que hasta ahora no ha visto el modelo.

Como con los sets de entrenamiento y validación, tenemos que pre-procesar el dataset (usando `map()` y la función `preprocesar_imagen`) y luego tomar lotes (de 16 imágenes) con `batch()`:
"""

# Preprocesar y hacer batches de test
test_batch = ds_test.map(preprocesar_imagen).batch(len(ds_test))
# Imprimir tamaño y categoría
for img, msk in test_batch:
    print(img.numpy().shape)
    print(msk.numpy().shape)

"""Se grafica tanto la función de pérdida como la exactitud del modelo con los datos de prueba"""

# Evaluar el modelo en el conjunto de prueba
test_loss, test_acc = unet.evaluate(test_batch, verbose=0)
print(f"Exactitud en el conjunto de prueba: {test_acc:.2f}")

# Graficar la precisión y la pérdida del entrenamiento
epochs = range(1, len(history.history['accuracy']) + 1)

plt.figure(figsize=(12, 5))

# Gráfico de la precisión
plt.subplot(1, 2, 1)
plt.plot(epochs, history.history['accuracy'], label='Exactitud en Entrenamiento')
plt.plot(epochs, history.history['val_accuracy'], label='Exactitud en Validación')
plt.title('Exactitud durante el Entrenamiento')
plt.xlabel('Épocas')
plt.ylabel('Exactitud')
plt.legend()

# Gráfico de la pérdida
plt.subplot(1, 2, 2)
plt.plot(epochs, history.history['loss'], label='Pérdida en Entrenamiento')
plt.plot(epochs, history.history['val_loss'], label='Pérdida en Validación')
plt.title('Pérdida durante el Entrenamiento')
plt.xlabel('Épocas')
plt.ylabel('Pérdida')
plt.legend()

plt.tight_layout()
plt.show()

"""Y ahora generamos las predicciones con el método `predict()`:"""

preds = unet.predict(test_batch) # Probabilidades

# Imprimir tamaño en pantalla
print(preds.shape)

"""Estas predicciones están almacenadas como un arreglo de NumPy y acá es importante tener en cuenta que cada una de las 554 imágenes predichas es de tamaño 128x128x6, es decir que se trata de imágenes de 6 canales.

Esto se debe a que cada pixel puede ser clasificado en una de 6 posibles categorías.

Cada uno de estos planos contiene la probabilidad (valor entre 0 y 1) de que cada pixel pertenezca a una categoría en particular.

Por ejemplo, tomemos por ejemplo la imagen 23 y veamos la distribución de probabilidades del pixel ubicado en la fila 13 columna 22:

Sin embargo, para poder visualizar estas máscaras de segmentación debemos "aplanar" estas imágenes a una dimensión, preservando la categoría de cada pixel
"""

preds[22,12,21,:]

"""Cada uno de los valores indica una probabilidad y para saber cuál es la categoría a la que pertenece este pixel simplemente debemos encontrar el plano que contiene el valor máximo, lo cual podemos calcular fácilmente usando `argmax()` de NumPy:"""

import numpy as np

# Las distribuciones de probabilidad
print(preds[22,12,21,:])

# La categoría a la que pertenece el pixel
np.argmax(preds[22,12,21,:])

"""El pixel pertenece a la categoría 5, lo cual quiere decir que la probabilidad más alta es la posición 6 dentro del vector de probabilidades.

Siguiendo esta lógica, podemos obtener las máscaras de segmentación de todas las imágenes de manera sencilla:
"""

# Máscaras de segmentación
# axis = 3: buscar el máximo de cada plano
mascaras = np.argmax(preds,axis=3)
print(mascaras.shape)

mascaras[30,:].shape

"""Ya tenemos 554 máscaras de segmentación (una por cada imagen de prueba) y cada una con un tamaño de 128x128 (el mismo tamaño de las imágenes originales).

Ahora sí podemos visualizar una imagen de prueba, la máscara original y la máscara predicha con el modelo entrenado.

Como las máscaras predichas están almacenadas como arreglos de NumPy, resulta más práctico convertir el lote de prueba (`test_batch`) a NumPy.

Esto podemos hacer sin correr el riesgo de tener problemas de almacenamiento en RAM, pues tan sólo hay 554 imágenes/máscaras de prueba.

Para hacer esta conversión:

1. Usamos `unbatch()` para deshacer los lotes creados previamente
2. Usamos `as_numpy_iterator()` para iterar sobre el dataset
3. El resultado de (1) y (2) lo almacenamos en una lista de Python
4. Finalmente iteramos sobre esta lista para almacenar las imágenes/máscaras en arreglos de NumPy:
"""

test_list = list(test_batch.unbatch().as_numpy_iterator())

# Almacenar imágenes y máscaras de prueba como arreglos de NumPy
imgs = np.asarray([item[0] for item in test_list])
msks = np.asarray([item[1] for item in test_list])

print(imgs.shape)
print(msks.shape)

"""¡Perfecto! Ya tenemos las imágenes y máscaras de prueba junto con las predicciones en formato NumPy. Ahora sí podemos ver algunas predicciones:"""

# Mostrar imagen, máscara y predicción

# Escoger una imagen aleatoriamente
id = np.random.randint(0,len(ds_test))

# Generar figura
fig, (ax1, ax2, ax3) = plt.subplots(1,3, figsize=(10,10))

# Imagen original
im1 = ax1.imshow(imgs[id])
ax1.set_title('Imagen original')
ax1.axis('off')
fig.colorbar(im1, ax=ax1)

# Máscara original
im2 = ax2.imshow(msks[id])
ax2.set_title('Máscara original')
ax2.axis('off')
fig.colorbar(im2, ax=ax2)

# Máscara predicha
im3 = ax3.imshow(mascaras[id])
ax3.set_title('Máscara predicha')
ax3.axis('off')
fig.colorbar(im3, ax=ax3)

