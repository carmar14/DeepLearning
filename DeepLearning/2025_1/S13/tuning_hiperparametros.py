# -*- coding: utf-8 -*-
"""tuning-hiperparametros.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/11XWRPOwYRmboKi9fVlVoRgVThfK9m1LR

# TUTORIAL: ¬øC√ìMO AFINAR LOS HIPER-PAR√ÅMETROS DE UNA RED NEURONAL CON KERAS TUNER?

En este tutorial veremos un tutorial b√°sico sobre c√≥mo usar Keras Tuner para afinar los hiperpar√°metros de una Red Neuronal.

Contenido:

1. [¬øQu√© son los hiperpar√°metros?](#scrollTo=A3bp1YrpGQAI&line=13&uniqifier=1)
2. [¬øQu√© es el ajuste o afinaci√≥n de hiperpar√°metros?](#scrollTo=3jGHchIPJOrL&line=9&uniqifier=1)
3. [El problema a resolver](#scrollTo=dljQnqglJtgV&line=5&uniqifier=1)
4. [El set de datos](#scrollTo=dEgFSAxkLce4&line=24&uniqifier=1)
5. [Pre-procesamiento del set de datos](#scrollTo=4gUZsJjdNa5i&line=11&uniqifier=1)
6. [üî•üî•üî•Uso de Keras Tunerüî•üî•üî•](#scrollTo=czFmIO7pU337&line=1&uniqifier=1)
7. [Generar predicciones con el modelo afinado](#scrollTo=BgXGkY69BTfZ&line=1&uniqifier=1)

## 1. ¬øQu√© son los hiperpar√°metros?


> Los hiperpar√°metros son un **conjunto de variables num√©ricas** que nosotros, como dise√±adores del modelo, **debemos escoger** correctamente para lograr obtener las mejores predicciones posibles.

Por ejemplo, en una Red Neuronal los hiperpar√°metros ser√≠an:

![](https://drive.google.com/uc?export=view&id=1rxHLiTwXVMdjNhdC70oSraWGwXwN36sb)

- El n√∫mero de capas ocultas
- El n√∫mero de neuronas en cada capa oculta
- La tasa de aprendizaje del optimizador

## 2. ¬øQu√© es el ajuste o afinaci√≥n de hiperpar√°metros?

> El ajuste de hiperpar√°metros consiste en **encontrar la mejor combinaci√≥n de hiperpar√°metros posible que maximice el desempe√±o del modelo**

En nuestro caso la idea es encontrar los mejores valores para el n√∫mero de capas ocultas, el n√∫mero de neuronas por capa y la tasa de aprendizaje del optimizador que permitan que la Red Neuronal genere las mejores predicciones.

Keras Tuner permite afinar estos hiperpar√°metros.

## 3. El problema a resolver

Para ver c√≥mo afinar los hiperpar√°metros usando Keras Tuner entrenaremos una sencilla Red Neuronal que tomar√° como entrada datos demogr√°ficos (edad, tipo de trabajo, nivel de educaci√≥n, estado civil, etc.) y que aprender√° a predecir si el nivel de ingresos de la persona ser√° menor o igual a 50.000 USD (0) o mayor a 50.000 USD (1):

![](https://drive.google.com/uc?export=view&id=1ryAI3C3nv4Y7Rj0uIdcaEvIh3o0yLYtP)

## 4. El set de datos

El set de datos se puede descargar desde [este enlace](https://drive.google.com/file/d/1g0JqE_qKh2qa_gSQqGAr_BHK6uduCLLN/view?usp=sharing).

El set de datos contiene un total de 48.842 registros de personas adultas en Estados Unidos.

Por cada persona se han recolectado estas variables:

1. `edad`
2. `tipo_trabajo`
3. `estim_pob` (poblaci√≥n estimada con el mismo perfil de la persona)
4. `nivel_educ`
5. `estado_civil`
6. `ocupaci√≥n`
7. `relaci√≥n` (parentesco que describe a la persona)
8. 	`incr_capital` (nivel de incremento del patrimonio en el a√±o anterior)
9. `perd_capital` (nivel de reducci√≥n del patrimonio en el a√±o anterior)
10.	`horas_semana` (n√∫mero de horas laboradas por semana)
11. `nacionalidad``
12. `ingresos` (menores o iguales a 50.000 USD - 0 - o mayores a 50.000 USD)

Las entradas al modelo ser√°n las variables 1 a 11 y el modelo deber√° aprender a predecir la variable 12 (ingresos).

Comencemos leyendo el set de datos. Despu√©s de descargarlo del enlace lo llevamos al disco duro de la m√°quina virtual de Google Colab y de all√≠ lo leemos con Pandas:
"""

# Leer dataset
import pandas as pd

df = pd.read_csv('/content/demografia_adultos.csv')
df

"""Hagamos una breve exploraci√≥n del set de datos.

Comencemos viendo las caracter√≠sticas de las variables num√©ricas:
"""

# Exploraci√≥n variales num√©ricas
df.describe()

"""Vemos que las variables num√©ricas (edad, estim_pob, nivel_educ, etc.) tienen rangos de valores diferentes. Antes de llevarlas a la Red Neuronal tendremos que escalarlas.

Hagamos ahora la exploraci√≥n de las variables categ√≥ricas (almacenadas como tipo `object` en el *DataFrame*):
"""

# Exploraci√≥n variables categ√≥ricas

# Listados para almacenar los nombres de las variables num√©ricas y categ√≥ricas
col_categoricas = []
col_numericas = []

# Iterar por las columnas del DataFrame
for columna in df.columns:

    # Si la columna es categ√≥rica
    if df[columna].dtype == 'object':
        # Imprimir los sub-niveles de la variable categ√≥rica
        print(f'{columna}: {df[columna].unique()}')

        # Y almacenar el nombre de la columna en el listado
        col_categoricas.append(columna)
    else:
        # Almacenar el nombre de la columna num√©rica en el listado
        col_numericas.append(columna)

"""## 5. Pre-procesamiento del set de datos

Para poder llevar los datos a la Red Neuronal tenemos que llevar a cabo varias etapas de pre-procesamiento. La idea es representar las variables en el formato num√©rico m√°s adecuado.

Estas ser√°n las etapas a implementar:

- Codificar las columnas categ√≥ricas en el formato one-hot (exceptuando la columna "ingresos")
- Codificar la columna categ√≥rica "ingresos" en formato binario (0: ingresos menores o iguales a 50.000, 1: ingresos mayores a 50.000)
- Crear los sets de entrenamiento, validaci√≥n y prueba
- Escalar las variables num√©ricas para que est√©n en el rango de 0 a 1
- Conformar los arreglos NumPy que usaremos a la entrada y salida de la Red Neuronal

Veamos cada una de estas fases.

### 5.1. Codificaci√≥n *one-hot* de las variables categ√≥ricas

La idea es representar cada variable categ√≥rica como una secuencia de 0s y 1s.

Por ejemplo, para la variable `estado_civil` la codificaci√≥n *one-hot* ser√≠a algo similar a esto:

`soltero(a)` ‚Üí [1, 0, 0]
`casado(a)` ‚Üí [0, 1, 0]
`divorciado(a)` ‚Üí [0, 0, 1]

Esto se puede lograr f√°cilmente en una sola l√≠nea de c√≥digo usando el m√©todo `get_dummies` de pandas y teniendo en cuenta que la codificaci√≥n la haremos para todas las variables categ√≥ricas **exceptuando "ingresos"**:
"""

col_categoricas

# Codificaci√≥n one-hot de las columnas categ√≥ricas exceptuando "ingresos"
df2 = pd.get_dummies(df,columns=col_categoricas[:-1])
df2.shape

"""### 5.2. Codificaci√≥n de la columna "ingresos" en formato binario

En este caso la idea es tomar esta columna y codificar sus dos niveles de la siguiente forma:

- `<=50K` ‚Üí 0
- `>50K` ‚Üí 1

Esto tambi√©n lo podemos hacer de forma muy sencilla usando `get_dummies()` pero agregando el argumento `drop_first=True` para que en lugar de codificar en formato *one-hot* ([1,0] y [0,1]) la codificaci√≥n sea binaria:
"""

# Codificar 0/1 columna ingresos
df2 = pd.get_dummies(df2, columns=['ingresos'], drop_first=True)
df2

df2.shape

"""### 5.3. Crear los sets de entrenamiento, validaci√≥n y prueba

- El set de entrenamiento permite obtener de forma autom√°tica los par√°metros del modelo
- üëâüëâüëâ**El set de validaci√≥n se usa para afinar los hiperpar√°metros del modelo**üëàüëàüëà
- El set de prueba se usa para generar predicciones con el modelo entrenado

En este caso haremos la partici√≥n de la siguiente forma:

- Entrenamiento: 70% de los datos
- Validaci√≥n: 20% de los datos
- Prueba: 10% restante

Comencemos mezclando aleatoriamente las filas del *DataFrame* (y fijando la semilla del generador aleatorio para que esta mezcla aleatoria sea siempre la misma cada vez que ejecutemos esta l√≠nea de c√≥digo):
"""

# Mezclar aleatoriamente (semilla = 123)
df_s = df2.sample(frac=1, random_state=123)
df_s

"""Ahora definamos el n√∫mero de datos que tendr√° cada sub-set con base en el tama√±o del set de datos original:"""

# N√∫mero de datos de los sets de entrenamiento, validaci√≥n y prueba
N = df_s.shape[0] # Cantidad total de datos
NTRAIN = int(0.7*N)
NVAL = int(0.2*N)
NTEST = N - NTRAIN - NVAL

"""Y ahora s√≠ seleccionamos los sets de entrenamiento, validaci√≥n y prueba a partir del *DataFrame* mezclado:"""

# Seleccionar las filas correspondientes
df_train = df_s.iloc[0:NTRAIN,:]
df_val = df_s.iloc[NTRAIN:NTRAIN+NVAL,:]
df_test = df_s.iloc[NTRAIN+NVAL:,:]

# Imprimir informaci√≥n en pantalla
print(f'Tama√±o set original: {df_s.shape}')
print(f'Tama√±o set de entrenamiento: {df_train.shape}')
print(f'Tama√±o set de validaci√≥n: {df_val.shape}')
print(f'Tama√±o set de prueba: {df_test.shape}')

"""Perfecto, ya tenemos 3 dataframes correspondientes a los sets de entrenamiento, validaci√≥n y prueba.

### 5.4. Escalar los datos num√©ricos

El siguiente paso es escalar los datos num√©ricos.

Como hemos usado la codificaci√≥n *one-hot* para las variables categ√≥ricas, estas tendr√°n valores entre 0 y 1.

As√≠ que resulta l√≥gico codificar los valores num√©ricos en el rango de 0 a 1 para lo cual usaremos `MinMaxScaler` de Scikit-Learn (de la cual hablo en detalle en un tutorial anterior).

Comencemos recordando cu√°les son las variables num√©ricas:
"""

df.describe()

"""Como cada variable num√©rica es diferente tendremos que usar un escalador para cada una de ellas.

Adem√°s, la l√≥gica de escalamiento ser√°:

- Se escala la variable de entrenamiento con `fit_transform()` y el resultado se almacena en el escalador correspondiente
- Se usa el resultado anterior junto con el m√©todo `transform()` para escalar la misma variable pero en los sets de validaci√≥n y prueba

"""

# No imprimir mensaje de advertencia
pd.options.mode.chained_assignment = None

# Importar MinMaxScaler
from sklearn.preprocessing import MinMaxScaler

# Diccionario que contendr√° los escaladores para cada variable num√©rica
scalers = {}

# Iterar por las variables num√©ricas (listado "col_numericas") y por cada variable:
# 1. Crear instancia del escalador y almacenarla en un diccionario
# 2. "fit_transform" sobre el set de entrenamiento
# 3. "transform" sobre los sets de validaci√≥n y prueba
for col in col_numericas:
    # 1. Instancia del escalador
    scalers[col] = MinMaxScaler()

    # 2. Fit-transform entrenamiento y reemplazar
    df_train.loc[:,col] = scalers[col].fit_transform(df_train[col].to_numpy().reshape(-1,1))

    # 3. Transform para escalar validaci√≥n y prueba
    df_val.loc[:,col] = scalers[col].transform(df_val[col].to_numpy().reshape(-1,1))
    df_test.loc[:,col] = scalers[col].transform(df_test[col].to_numpy().reshape(-1,1))

# Verificar
print(f'Entrenamiento min/max: {df_train.min().min()}/{df_train.max().max()}')
print(f'Validaci√≥n min/max: {df_val.min().min()}/{df_val.max().max()}')
print(f'Prueba min/max: {df_test.min().min()}/{df_test.max().max()}')

"""### 5.5. Conformar arreglos de entrada y de salida a la Red Neuronal

Hasta ahora tenemos los sets de entrenamiento, validaci√≥n y prueba en formato *DataFrame* de Pandas.

Pero para llevarlos a la Red Neuronal de Keras tendremos que convertirlos al formato NumPy.

En primer lugar observemos los tipos de dato que tenemos en los *DataFrames*:
"""

df_train.info()

"""Todos son datos num√©ricos tipo `float64` (n√∫meros decimales) o `uint8` (n√∫meros enteros).

Sin embargo, lo mejor es representar todas las variables como datos num√©ricos en formato punto flotante (`float64`).

Veamos ahora el tama√±o de los *DataFrames*:
"""

df_val

print(df_train.shape)
print(df_val.shape)
print(df_test.shape)

"""Son arreglos de 79 columnas. Las primeras 78 ser√°n las variables de entrada al modelo (las llamaremos "x") mientras que la √∫ltima columna ser√° la variable a predecir (la llamaremos "y").


As√≠ que por cada *DataFrame* (`df_train`, `df_val` y `df_test`) haremos lo siguiente:

- Crearemos el arreglo *x* que ser√° la entrada al modelo y que tomar√° las primeras 78 columnas
- Crearemos el arreglo *y* que ser√° la salida que debe aprender a predecir el modelo y que corresponde a la √∫ltima columna
- Para extraer los valores en formato NumPy usaremos el m√©todo `to_numpy` y agregaremos el argumento `dtype=np.float64` para convertir los valores al formato de punto flotante adecuado para Keras

Veamos c√≥mo implementar estos pasos:
"""

# Conformar arreglos X, Y
import numpy as np

# Train
x_train = df_train.iloc[:,:78].to_numpy(dtype=np.float64)
y_train = df_train.iloc[:,78].to_numpy(dtype=np.float64)

# Val
x_val = df_val.iloc[:,:78].to_numpy(dtype=np.float64)
y_val = df_val.iloc[:,78].to_numpy(dtype=np.float64)

# Test
x_test = df_test.iloc[:,:78].to_numpy(dtype=np.float64)
y_test = df_test.iloc[:,78].to_numpy(dtype=np.float64)

# Verificar tama√±os
print(f'Entrenamiento (x,y): {x_train.shape}, {y_train.shape}')
print(f'Validaci√≥n (x,y): {x_val.shape}, {y_val.shape}')
print(f'Prueba (x,y): {x_test.shape}, {y_test.shape}')

"""¬°Perfecto, en este punto ya tenemos los datos listos para presentarlos al modelo!

## 6. üî•üî•üî•Uso de Keras Tunerüî•üî•üî•

Recordemos que queremos afinar los hiperpar√°metros de la Red Neuronal.

Y estos hiperpar√°metros son:

1. El **n√∫mero de capas ocultas**: probaremos con modelos de 1, 2 o 3 capas ocultas
2. El **n√∫mero de neuronas por capa oculta**: probaremos con 20, 40 o 60 neuronas por capa
3. La **tasa de aprendizaje del optimizador**: probaremos con tasas de aprendizaje de 0.1, 0.01 y 0.001
3. Las **funciones de activaci√≥n de las neuronas en cada capa**: probaremos con tanh y relu

As√≠ que la afinaci√≥n consiste en:

1. Crear un modelo con cada combinaci√≥n de hiperpar√°metros
2. Entrenar y validar dicho modelo y almacenar los resultados
3. Repetir los pasos (1) y (2) para cada combinaci√≥n
4. Escoger el modelo con el mejor desempe√±o

En este caso podremos tener un total de 54 diferentes combinaciones de hiperpar√°metros, pues cada uno puede tener 3 valores diferentes (3 diferentes n√∫meros de capas ocultas, 3 diferentes n√∫meros de neuronas por capa, 3 diferentes tasas de aprendizaje) y 2 para las funciones de activaci√≥n.

Aunque suena complicado en realidad esta afinaci√≥n resulta muy sencilla con Keras Tuner.

Para poder afinar el modelo usando Keras Tuner debemos llevar a cabo 2 pasos:

- Paso 1: crear una funci√≥n que acepte como entrada un objeto tipo `HyperParameters()` y que nos permitir√° f√°cilmente crear diferentes modelos para los diferentes hiperpar√°metros a afinar
2. Paso 2: crear un afinador (*tuner*) definiendo los criterios de afinaci√≥n y usarlo para entrenar y escoger el mejor modelo tras esta afinaci√≥n.

Veamos en detalle cada uno de estos pasos

### 6.1. Paso 1: escribir la funci√≥n `crear_modelo`

Esta funci√≥n:

- Toma como entrada un objeto tipo `HyperParameters()`.
- El objeto tipo `HyperParameters()` contendr√° los hiperpar√°metros con los que construiremos el modelo.
- La funci√≥n toma estos hiperpar√°metros y a la salida retorna el modelo con los hiperpar√°metros especificados

Veamos c√≥mo implementar esta funci√≥n, pero antes debemos instalar Keras Tuner:
"""

#!pip install keras-tuner -q

"""Ahora s√≠ implementemos la funci√≥n, fijando inicialmente la semilla del generador aleatorio para la reproducibilidad del entrenamiento:"""

import tensorflow as tf

# Semilla generador aleatorio
tf.random.set_seed(23)

def crear_modelo(hp):
    # Contenedor vac√≠o
    modelo = tf.keras.Sequential()

    # Capa de entrada: cada dato tendr√° 78 elementos
    modelo.add(tf.keras.layers.Input(shape=(78,)))

    # =======
    # Hiperpar√°metro n√∫mero de capas ocultas (1, 2 o 3)
    for i in range(hp.Int("num_layers",
                             min_value=1, # M√≠nimo n√∫mero de capas ocultas
                             max_value=3, # M√°ximo n√∫mero de capas ocultas
                             step=1 # Tama√±o del salto (para obtener 1, 2 o 3)
                          )):
        # =======
        # Hiperpar√°metro n√∫mero de neuronas por capa
        modelo.add(tf.keras.layers.Dense(
            units = hp.Int(f"units_{i}",
                           min_value=20, # M√≠nimo n√∫mero de neuronas/capa
                           max_value=60, # M√°ximo n√∫mero de neuronas/capa
                           step=20 # Tama√±o del salto (para obtener 20, 40, 60)
                           ),
            activation=hp.Choice("activation", ["relu", "tanh"]),)
        )

    # Capa de salida
    modelo.add(tf.keras.layers.Dense(units=1, activation="sigmoid"))

    # =======
    # Hiperpar√°metro tasa de aprendizaje del optimizador
    hp_lr = hp.Choice("learning_rate", values=[0.1, 0.01, 0.001])

    # Compilar (optimizador, p√©rdida, m√©trica de desempe√±o)
    modelo.compile(
        optimizer = tf.keras.optimizers.SGD(learning_rate=hp_lr),
        loss=tf.keras.losses.BinaryCrossentropy(),
        metrics=["accuracy"])

    return modelo

"""Y listo, ya hemos creado la funci√≥n para construir el modelo.

Esta funci√≥n ser√° llamada por el afinador (*tuner*) y cada vez que la llame se usar√° una combinaci√≥n diferente de hiperpar√°metros, generando por tanto un modelo diferente cada vez que sea ejecutada.

Verifiquemos que al llamar la funci√≥n no ocurre ning√∫n error:
"""

# Verificar si la funci√≥n construye el modelo correctamente
import keras_tuner

crear_modelo(keras_tuner.HyperParameters())

"""¬°Perfecto! Ya tenemos el primer paso. Veamos ahora el segundo paso: c√≥mo crear y usar el afinador para entrenar nuestro modelo.

### 6.2. Paso 2: crear usar el *tuner* para afinar los hiperpar√°metros

Recordemos que tenemos 27 posibles combinaciones de hiper-par√°metros, as√≠ que la idea ser√≠a entrenar y validar 27 diferentes Redes Neuronales y luego escoger aquella con los hiperpar√°metros que generen el mejor desempe√±o.

En nuestro caso es posible (pues el set de datos y el modelo son muy simples) pero en la pr√°ctica esto no es viable pues se requerir√≠an demasiadas capacidades computacionales y demasiado tiempo de afinaci√≥n.

As√≠ que en la afinaci√≥n de hiper-par√°metros existen tres enfoques de b√∫squeda de los mejores hiperpar√°metros:

- *RandomSearch*: de todas las posibles combinaciones se escogen aleatoriamente algunas de ellas y se afina el modelo con base en esa selecci√≥n de combinaciones
- *BayesianOptimization*: se usan resultados de combinaciones anteriores para buscar la siguiente combinaci√≥n que tendr√° mejores probabilidades de mejorar el desempe√±o
- *Hyperband*: se entrenan m√∫ltiples modelos con pocas iteraciones y los de peor desempe√±o se eliminan para luego afinar los modelos m√°s "prometedores" usando m√°s iteraciones

En este caso usaremos "RandomSearch". Comencemos entonces creando el afinador (*tuner*) para este tipo de b√∫squeda:
"""

tuner = keras_tuner.RandomSearch(
    hypermodel=crear_modelo,  # La funci√≥n que creamos anteriormente
    objective="val_accuracy", # La m√©trica de desempe√±o a usar en la afinaci√≥n
    max_trials=10, # N√∫mero total de combinaciones a probar
    directory="afinacion_red", # Directorio local donde se almacenar√°n los resultados de entrenamiento
    project_name="trials", # Subdirectorio donde se almacenar√°n los resultados
    overwrite = True, # Para sobre-escribir los directorios cada vez que afinemos
)

"""Al hacer lo anterior podemos ver que se crea el directorio "afinacion_red" en el disco duro de la m√°quina virtual.

Podemos usar el m√©todo `search_space_summary()` para imprimir en pantalla la informaci√≥n general del afinador:
"""

# Imprimir en pantalla el espacio de b√∫squeda
tuner.search_space_summary()

"""En el caso anterior vemos que el tama√±o del espacio de b√∫squeda es 4 porque queremos afinar 4 hiperpar√°metros.

Adem√°s se imprime en pantalla el rango de valores que tomar√° cada hiperpar√°metro durante la afinaci√≥n.

Y ahora lo √∫nico que nos falta es afinar el modelo, para lo cual usamos el m√©todo `search()`.

Este m√©todo:
- Crear√° un modelo a partir de la funci√≥n `crear_modelo` y usando una combinaci√≥n aleatoria de hiperpar√°metros
- Tomar√° los sets de entrenamiento (`x_train`, `y_train`) y validaci√≥n (`x_val`, `y_val`), se los presentar√° al modelo y lo entrenar√°
- Almacenar√° el modelo entrenado en la carpeta "afinacion_red" as√≠ como su desempe√±o
- Y repetir√° el proceso el n√∫mero de veces especificado en `max_trials`

Y al final de este proceso tendremos un total de `max_trials` modelos y f√°cilmente podremos escoger el mejor de todos.

Veamos c√≥mo realizar la afinaci√≥n usando una sola l√≠nea de c√≥digo!:
"""

# Afinar los hiperpar√°metros
tuner.search(x_train, y_train,
             epochs=3,
             validation_data=(x_val, y_val))

"""Observamos que en cada "trial" se imprime en pantalla:

- El tiempo de ejecuci√≥n del "trial"
- La exactitud obtenida con ese "trial" para el set de validaci√≥n
- La mejor exactitud con el set de validaci√≥n obtenida hasta el momento
- Una peque√±a tabla con los valores de los hiperpar√°metros en este "trial" y los mejores valores encontrados hasta el momento

Para modelos relativamente grandes se sugiere usar la GPU de Google Colab para acelerar la afinaci√≥n.

Una vez completada la afinaci√≥n podemos ver que la carpeta "afinacion_red" y la sub-carpeta "trials" contienen los diferentes modelos entrenados as√≠ como el resumen de los mismos.

Adem√°s podemos imprimir en pantalla un resumen de esta afinaci√≥n usando el m√©todo `results_summary()`:
"""

# Imprimir resumen de la afinaci√≥n
tuner.results_summary()

"""Estos resultados aparecen organizados de mayor a menor, es decir del modelo m√°s exitoso (con mejor desempe√±o) al menos exitoso (con el peor desempe√±o).

Por cada uno de los modelos se imprimen los hiperpar√°metros correspondientes as√≠ como el desempe√±o obtenido.

Y finalmente, s√≥lo nos resta escoger el mejor modelo usando el m√©todo `get_best_models`:
"""

# Extraer el mejor modelo
mejor_modelo = tuner.get_best_models(num_models=1)[0]
mejor_modelo.summary()

"""Y listo, hemos visto que realmente se requieren poqu√≠simas l√≠neas de c√≥digo para afinar un modelo usando Keras Tuner.

S√≥lo nos resta usar este mejor modelo para generar predicciones sobre nuevos datos.

## 7. Generar predicciones con el modelo afinado

Este es el paso m√°s sencillo de todos, simplemente usamos el modelo que acabamos de seleccionar (`mejor_modelo`) y el m√©todo `predict` para generar predicciones sobre nuevos datos.

En este caso recordemos que los datos deben estar pre-procesados y que esta fase ya la implementamos antes.

Por tanto podemos usar `x_test` para generar las predicciones con el mejor modelo:
"""

# Predicciones sobre el set de prueba
logits = mejor_modelo.predict(x_test)
preds = (logits > 0.5).astype(int)
print(preds)

"""¬°Y listo! Ya hemos visto c√≥mo afinar una Red Neuronal usando Keras Tuner y como usar el modelo afinado para generar predicciones sobre nuevos datos."""